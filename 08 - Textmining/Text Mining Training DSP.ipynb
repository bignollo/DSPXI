{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T00:04:33.618400Z",
     "start_time": "2019-04-08T00:04:33.455167Z"
    }
   },
   "source": [
    "![image.png](https://raw.githubusercontent.com/fjvarasc/DSPXI/master/figures/py_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Text-Mining\" data-toc-modified-id=\"Text-Mining-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Text Mining</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Useful-Links\" data-toc-modified-id=\"Useful-Links-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Useful Links</a></span></li></ul></li><li><span><a href=\"#Regular-Expressions\" data-toc-modified-id=\"Regular-Expressions-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Regular Expressions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Useful-Links\" data-toc-modified-id=\"Useful-Links-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Useful Links</a></span></li><li><span><a href=\"#Let's-play\" data-toc-modified-id=\"Let's-play-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Let's play</a></span></li></ul></li><li><span><a href=\"#Text-Preprocessing\" data-toc-modified-id=\"Text-Preprocessing-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Text Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Noise-Removal\" data-toc-modified-id=\"Noise-Removal-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Noise Removal</a></span></li><li><span><a href=\"#Tokenization\" data-toc-modified-id=\"Tokenization-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Tokenization</a></span></li><li><span><a href=\"#Stopwords\" data-toc-modified-id=\"Stopwords-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>Stopwords</a></span></li><li><span><a href=\"#Stemming\" data-toc-modified-id=\"Stemming-1.3.4\"><span class=\"toc-item-num\">1.3.4&nbsp;&nbsp;</span>Stemming</a></span></li><li><span><a href=\"#Lemming\" data-toc-modified-id=\"Lemming-1.3.5\"><span class=\"toc-item-num\">1.3.5&nbsp;&nbsp;</span>Lemming</a></span></li><li><span><a href=\"#NLTK\" data-toc-modified-id=\"NLTK-1.3.6\"><span class=\"toc-item-num\">1.3.6&nbsp;&nbsp;</span>NLTK</a></span></li><li><span><a href=\"#SpaCy\" data-toc-modified-id=\"SpaCy-1.3.7\"><span class=\"toc-item-num\">1.3.7&nbsp;&nbsp;</span>SpaCy</a></span></li><li><span><a href=\"#Let's-play\" data-toc-modified-id=\"Let's-play-1.3.8\"><span class=\"toc-item-num\">1.3.8&nbsp;&nbsp;</span>Let's play</a></span></li></ul></li><li><span><a href=\"#Document-Term-Matrix\" data-toc-modified-id=\"Document-Term-Matrix-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Document Term Matrix</a></span><ul class=\"toc-item\"><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>TF-IDF</a></span></li></ul></li><li><span><a href=\"#Asignment\" data-toc-modified-id=\"Asignment-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Asignment</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text mining, sometimes also referred as Text analytics, is the process of deriving information from text. The objective is to extract useful information from text based content. This content can be a an email, a comment on social media, a review, a scientific paper, a contract, a book, and so on.  \n",
    "Some useful applications for text mining are:\n",
    "* Check popularity of a topic on social media\n",
    "* Evaluate a review of a product as positive or negative\n",
    "* Summarize the content of multiple news sources\n",
    "* Surveillance of email to prevent fraud\n",
    "* Extract important information from a contract\n",
    "\n",
    "Text Mining can also be used to turn unstructured data into structured data. Qualitative data or unstructured data are data that cannot be measured in terms of numbers. These data usually contain information like colour, texture and text. Quantitative data or structured data are data that can be measured easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Links\n",
    "* [Wikipedia Article about Text Mining](https://en.wikipedia.org/wiki/Text_mining)\n",
    "* [A Definitive Guide on How Text Mining Works](https://www.educba.com/text-mining/)\n",
    "* [About Text Mining (IBM article)](https://www.ibm.com/support/knowledgecenter/en/SS3RA7_17.1.0/ta_guide_ddita/textmining/shared_entities/tm_intro_tm_defined.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Regular Expression](https://en.wikipedia.org/wiki/Regular_expression) (usually expressed as **RegEx**) is a way of finding and/or replacing text by matching patterns. Opposite to regular text finding, where we want to match an exact string or character, in RegEx we want to find an specific pattern. Examples of data that follow a pattern:\n",
    "* Telephone Numbers\n",
    "* Document Numbers (RUT, Passport)\n",
    "* Car Plates\n",
    "* Dates\n",
    "* URLs\n",
    "* Emails\n",
    "\n",
    "The website RegEx101 provides a summary of the regex patterns and an easy way to test an expression.  \n",
    "For Python, we are going to use the [re](https://docs.python.org/3/library/re.html) package.\n",
    "This package has a function, findall, that returns all occurrences of a regular expression in a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eduardo.lopes@evalueserve.com', 'compliance@evalueserve.com']"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Text to be searched\n",
    "sample_text = 'You can send the details for eduardo.lopes@evalueserve.com. \\\n",
    "                Please keep in copy compliance@evalueserve.com'\n",
    "\n",
    "# Pattern to be found\n",
    " \n",
    "\n",
    "# Return all occurences of email_pattern in sample_text\n",
    "re.findall(email_pattern, sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, we are saying to Python that we are looking for a sequence of lowercase letters and/or dots (**[a-z.]**) of any length (the \\* ), followed by an **@***, followed then by another sequence of lowercase letters (**[a-z]**), ending with **.com**.\n",
    "\n",
    "To help build our own Regular Expression, there are some pre-defined characters classes for the most used cases.  \n",
    "Let's explore them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic Character Classes**\n",
    "\n",
    "These are the basic predefined classes.\n",
    "\n",
    "| Symbol | Matches                                 |\n",
    "|--------|-----------------------------------------|\n",
    "|\\d     |Any digit                               |    \n",
    "|\\w     |Any Alphanumeric character (Includes _) |    \n",
    "|\\s     |Any Whitespaces                     |        \n",
    "|\\t     |Tab character                           |    \n",
    "|\\n     |New line character                      |    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '9', '9', '1']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = 'email1991@evalueserve.com compliance@evalueserve.com'\n",
    "re.findall(r'\\d', sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Negation Character Classes**\n",
    "\n",
    "These are the negation of the classes above. As we can see, hey are the representation of the respective class, but in **UPPERCASE**.\n",
    "\n",
    "| Symbol | Matches                        |\n",
    "|--------|--------------------------------|\n",
    "| \\D     | Any non-digit                  |\n",
    "| \\W     | Any non-Alphanumeric character |\n",
    "| \\S     | Any non-whitespaces            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@', '.', ' ', '@', '.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = 'email1991@evalueserve.com compliance@evalueserve.com'\n",
    "re.findall(r'\\W', sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User Defined Character Classes**\n",
    "\n",
    "If we want a specific set of characters (just vowels, for instance), we can use this class to define our own group.  \n",
    "We used that in the Email example above.\n",
    "\n",
    "| Symbol         | Matches                                         |\n",
    "|----------------|-------------------------------------------------|\n",
    "| [abc]          | a, b or c                                       |\n",
    "| [ab] \\| [c]          | Or operator, equivalent to above (a, b or c)                                       |\n",
    "| [^abc]         | Negation, matches everything except a, b, or c. |\n",
    "| [a-c]          | Range, matches anything between a and c         |\n",
    "| [a-c[f-h]]     | Union, matches a, b, c, f, g, h                 |\n",
    "| [a-c&&[b-c]]   | Intersection, matches b or c                    |\n",
    "| [a-c&&[^b-c]]  | Subtraction, matches a                          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e',\n",
       " 'a',\n",
       " 'i',\n",
       " '1',\n",
       " '9',\n",
       " '9',\n",
       " '1',\n",
       " 'e',\n",
       " 'a',\n",
       " 'u',\n",
       " 'e',\n",
       " 'e',\n",
       " 'e',\n",
       " 'o',\n",
       " 'o',\n",
       " 'i',\n",
       " 'a',\n",
       " 'e',\n",
       " 'e',\n",
       " 'a',\n",
       " 'u',\n",
       " 'e',\n",
       " 'e',\n",
       " 'e',\n",
       " 'o']"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = 'email1991@evalueserve.com compliance@evalueserve.com'\n",
    "re.findall(r'[aeiou]|[0-9]', sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quantifiers**\n",
    "\n",
    "Quantifiers are operators used in combination with the classes to determine the length of the sequence we are interested.\n",
    "For instance, if we are looking for years, we are only interestd in sequence of digits of length 4.\n",
    "\n",
    "| Symbol  | Matches                                         | Example |\n",
    "|---------|-------------------------------------------------|---------|\n",
    "| ? | Zero or one ocurrences |   \\d?      |\n",
    "| * | Zero or more ocurrences |   \\d*      |\n",
    "| + | One or more ocurrences |    \\d+     |\n",
    "| {n}   | A sequence of length n               |  \\d{n}        |\n",
    "| {n,}  | A sequence length at least n      |   \\d{n,}      |\n",
    "| {n,m} | A sequence of length between n and m |   \\d{n,m}      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1991']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = 'email1991@evalueserve.com compliance@evalueserve.com'\n",
    "re.findall(r'[0-9]{3,}', sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other Operators**\n",
    "\n",
    "Here we introduced the most commom RegEx operators, but there are many more.  \n",
    "You can find a list of all the operators available in [Regex101](https://regex101.com/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Links\n",
    "* [Wikipedia article about RegEx](https://en.wikipedia.org/wiki/Regular_expression)\n",
    "* [Regex101](https://regex101.com/)\n",
    "* [re documentation](https://docs.python.org/3/library/re.html)\n",
    "* [Regex on Python](https://www.w3schools.com/python/python_regex.asp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to our previous example, can we make a better regular expression to identify emails?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eduardo.lopes@evalueserve.com', 'compliance@evalueserve.com']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = 'You can send the details for eduardo.lopes@evalueserve.com. Please keep in copy compliance@evalueserve.com'\n",
    "email_pattern = r'[a-z.]*@[a-z]*.com' # Put your pattern here\n",
    "re.findall(email_pattern, sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with text, we usually have to deal with sequence of characters where most of them are not interesting for analysis (letters, numbers, punctuation, especial characters, blanks, spaces, etc). Because of that,  in every text mining/analytics project, the first step is the preprocessing.  \n",
    "Usually the Text Preprocessing involves the folowing steps:\n",
    "* Noise Removal\n",
    "* Tokenization\n",
    "* Stopword Removal\n",
    "* Stemming\n",
    "* Lemmatization\n",
    "\n",
    "Depending on the data available and the objective of the analysis, some of these steps can be skipped and additional steps could be needed.  \n",
    "Now, we will walk through each one of these steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noise removal is the process of removing unwanted characters such as:  \n",
    "* Text file headers and/or footers \n",
    "* HTML, XML, etc. markup and metadata  \n",
    "\n",
    "Usually it is necessary when dealing with content extracted from the web. Because of that, noise removal is a more specific, that depends on the kind of text we are dealing with.  \n",
    "\n",
    "For the next example, we are using the [urllib](https://docs.python.org/3/library/urllib.html) package to download a html page from the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information_retrieval\" title=\"Information retrieval\">information retrieval</a>, <a href=\"/wiki/Lexical_analysis\" title=\"Lexical analysis\">lexical analysis</a> to study word frequency distributions, <a href=\"/wiki/Pattern_recognition\" title=\"Pattern recognition\">pattern recognition</a>, <a href=\"/wiki/Tag_(metadata)\" title=\"Tag (metadata)\">tagging</a>/<a href=\"/wiki/Annotation\" title=\"Annotation\">annotation</a>, <a href=\"/wiki/Information_extraction\" title=\"Information extraction\">information extraction</a>, <a href=\"/wiki/Data_mining\" title=\"Data mining\">data mining</a> techniques including link and association analysis, <a href=\"/wiki/Information_visualization\" title=\"Information visualization\">visualization</a>, and <a href=\"/wiki/Predictive_analytics\" title=\"Predictive analytics\">predictive analytics</a>. The overarching goal is, essentially, to turn text into data for analysis, via application of <a href=\"/wiki/Natural_language_processing\" title=\"Natural language processing\">natural language processing</a> (NLP) and analytical methods.\n",
      "</p><p>A typical application is to scan a set of documents written in a <a href=\"/wiki/Natural_language\" title=\"Natural language\">natural language</a> and either model the document set for <a href=\"/wiki/Predictive_classification\" class=\"mw-redirect\" title=\"Predictive classification\">predictive classification</a> purposes or populate a database or search index with the information extracted.\n",
      "</p>\n",
      "<div id=\"toc\" class=\"toc\"><input type=\"checkbox\" role=\"button\" id=\"toctogglecheckbox\" class=\"toctogglecheckbox\" style=\"display:none\" /><div class=\"toctitle\" lang=\"en\" dir=\"ltr\"><h2>Contents</h2><span class=\"toctogglespan\"><label class=\"toctogglelabel\" for=\"toctogglecheckbox\"></label></span></div>\n",
      "<ul>\n",
      "<li class=\"toclevel-1 tocsection-1\"><a href=\"#Text_analytics\"><span class=\"tocnumber\">1</span> <span class=\"toctext\">Text analytics</span></a></li>\n",
      "<li class=\"toclevel-1 tocsection-2\"><a href=\"#Text_analysis_processes\"><span class=\"tocnumber\n"
     ]
    }
   ],
   "source": [
    "# Import package\n",
    "from urllib import request\n",
    "\n",
    "# Download html web page\n",
    "url = \"https://en.wikipedia.org/wiki/Text_mining\"\n",
    "html = request.urlopen(url).read().decode('utf8')\n",
    "\n",
    "# Print a sample of the html\n",
    "print(html[10000:12000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can, see there are a lot of undesired text, mostly html tags. To continue with our analysis, we need to remove this unwanted data.  \n",
    "Luckily, there is a Python package that can help us with that, [Beatiful Soup](https://pypi.org/project/beautifulsoup4/). It is a package for pulling data out of HTML and XML files. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business problems, whether independently or in conjunction with query and analysis of fielded, numerical data. It is a truism that 80 percent of business-relevant information originates in unstructured form, primarily text.[5] These techniques and processes discover and present knowledge – facts, business rules, and relationships – that is otherwise locked in textual form, impenetrable to automated processing.\n",
      "\n",
      "Text analysis processes[edit]\n",
      "Subtasks—components of a larger text-analytics effort—typically include:\n",
      "\n",
      "Information retrieval or identification of a corpus is a preparatory step: collecting or identifying a set of textual materials, on the Web or held in a file system, database, or content corpus manager, for analysis.\n",
      "Although some text analytics systems apply exclusively advanced statistical methods, many others apply more extensive natural language processing, such as part of speech tagging, syntactic parsing, and other types of linguistic analysis.[citation needed]\n",
      "Named entity recognition is the use of gazetteers or statistical techniques to identify named text features: people, organizations, place names, stock ticker symbols, certain abbreviations, and so on.\n",
      "Disambiguation—the use of contextual clues—may be required to decide where, for instance, \"Ford\" can refer to a former U.S. president, a vehicle manufacturer, a movie star, a river crossing, or some other entity.\n",
      "Recognition of Pattern Identified Entities: Features such as telephone numbers, e-mail addresses, quantities (with units) can be discerned via regular expression or other pattern matches.\n",
      "Document clustering: identification of sets of similar text documents.[6]\n",
      "Coreference: identification of noun phrases and other terms that refer to the same object.\n",
      "Relationship, fact, and event Extraction: identification of associations among entities and other information in text\n",
      "Sentiment analysis involves discerning subjective (as opposed to factual) material and extracting various forms of attitudin\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "print(soup.get_text()[8000:10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the html tags removed, we are left with pure text. So now we can move on in our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the act of spliting longer strings of text into smaller pieces, or **tokens**. Large documents can be tokenized into sentences, sentences can be tokenized into words, etc. In some situations, a name composed of two or more words can be considered a single token (Banf of America, Great Britain, Vina del Mar, etc). \n",
    "\n",
    "Tokenization make further analysis, like counting words or summarizing information, easier and faster to perform.\n",
    "\n",
    "To understand how it is done, let's tokenize our first sentence.We will use the function split from the re package. This function split a text for a given regular expression.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To', 'understand', 'how', \"it's\", 'done,', \"let's\", 'tokenize', 'our', 'first', 'sentence.', \"Don't\", 'forget', 'special', 'cases,', 'like', 'U.S.A.', 'This', 'is', 'important', 'for', 'understanding,', 'specially', 'when', 'learning']\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"To understand how it's done, let's tokenize our first sentence. Don't forget special cases, like U.S.A. This is important for understanding, specially when learning\"\n",
    "\n",
    "# Split ouourr text in whitespaces\n",
    "split_pattern = r'\\s'\n",
    "tokens = re.split(split_pattern, sample_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our first list of tokens.  \n",
    "Unfortunatelly, it resulted in some undesirable tokens:\n",
    "* **done,** (comma at the end)\n",
    "* **sentence.** (period at the end)\n",
    "\n",
    "Also, there are some special cases that some applications find acceptable. We will explore these later.\n",
    "* **it's**\n",
    "* **let's**\n",
    "* **Don't**\n",
    "* **U.S.A.**\n",
    "\n",
    "One alternative is to look for anything that is not an alphanumeric character, and split the text in that position.\n",
    "RegEx has an operator for that, the **\\W** class. We will include the quantifier operator **+** to identify sequence of one or more characters, so sequence of whitesapaces or special characters folowed by a whitespace will be counted as one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To', 'understand', 'how', 'it', 's', 'done', 'let', 's', 'tokenize', 'our', 'first', 'sentence', 'Don', 't', 'forget', 'special', 'cases', 'like', 'U', 'S', 'A', 'This', 'is', 'important', 'for', 'understanding', 'specially', 'when', 'learning']\n"
     ]
    }
   ],
   "source": [
    "# Split text in whitespaces\n",
    "split_pattern = r'\\W+'\n",
    "tokens = re.split(split_pattern, sample_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to correct the undesirable tokens, but the special cases (it's, let's, Don't were U.S.A.) were split.  \n",
    "So we need to think about a more complex pattern.  \n",
    "We can make a regular expressio to each special case, and then concatenate them with the OR (**|**) operator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To', 'understand', 'how', \"it's\", 'done', \"let's\", 'tokenize', 'our', 'first', 'sentence', \"Don't\", 'forget', 'special', 'cases', 'like', 'U.S.A', 'This', 'is', 'important', 'for', 'understanding', 'specially', 'when', 'learning']\n"
     ]
    }
   ],
   "source": [
    "pattern = r'''(?x)         # set flag to allow regexps over multiple lines\n",
    "            \\w[.]\\w[.]\\w   # U.S.A.\n",
    "            | \\w+[']\\w     # it's, let's, Don't\n",
    "            | \\w+          # other words\n",
    "        '''\n",
    "tokens = re.findall(pattern, sample_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This appear to work with our example, but still is not the perfect solution. For other abbreviations or word with hyphens (drive-thru), our tokenizer won't work properly.  \n",
    "To help us with that task, the Natural Language Toolkit ([NLTK](http://www.nltk.org/)) package has a tokenizer. This package is focused on text mining, and will be used in our next steps in pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To', 'understand', 'how', 'it', \"'s\", 'done', ',', 'let', \"'s\", 'tokenize', 'our', 'first', 'sentence', '.', 'Do', \"n't\", 'forget', 'special', 'cases', ',', 'like', 'U.S.A', '.', 'This', 'is', 'important', 'for', 'understanding', ',', 'specially', 'when', 'learning']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(sample_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer from NLTK, diferent from the one we built with RegEx, separates the words with apostrophe (\\').  \n",
    "The NLTK tokenizer is equivalent to the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To', 'understand', 'how', 'it', \"'\", 's', 'done', ',', 'let', \"'\", 's', 'tokenize', 'our', 'first', 'sentence', '.', 'Don', \"'\", 't', 'forget', 'special', 'cases', ',', 'like', 'U.S.A.', 'This', 'is', 'important', 'for', 'understanding', ',', 'specially', 'when', 'learning']\n"
     ]
    }
   ],
   "source": [
    "pattern = r'''(?x)          # set flag to allow regexps over multiple lines\n",
    "        (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "      | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
    "      | \\$?\\d+(?:\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "      | \\.\\.\\.              # ellipsis\n",
    "      | [][.,;\"'?():_`-]    # these are separate tokens; includes ], [\n",
    "    '''\n",
    "tokens = re.findall(pattern, sample_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, tokenization is not an easy task. It involves not only technical skills, but also an understanding of the language and the problem to be solved. Specially when dealing with specific subjects (scientific research, for instance), we can stumble accros some domain specific terms, acronomys, abbreviations, etc \n",
    "\n",
    "> **\"It is not safe to make the assumption that source text will be perfect. A tokenizer must often be customized to the data in question.\"** \n",
    ">>*Natural Language Processing with Python - Analyzing Text with the Natural Language Toolkit  \n",
    ">>Steven Bird, Ewan Klein, and Edward Loper - NLTK package creators*\n",
    "\n",
    "That's why NLTK tokenizator is an option, but not the only one, as we can see below:\n",
    "\n",
    "![Table comparing different Tokenizers](https://raw.githubusercontent.com/fjvarasc/DSPXI/master/figures/TokenizerComparison.png \"Comparing different Tokenizers\")\n",
    "*source: https://www.ibm.com/developerworks/community/blogs/nlp/entry/tokenization?lang=en*\n",
    "\n",
    "**Useful Link**\n",
    "* [Compare different tokenization methods](https://text-processing.com/demo/tokenize/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stop words** are the most common words in a language like “the”, “a”, “on”, “is”, “all”. These words do not carry important meaning and are usually removed from texts. NLTK has a built-in stopwords dictionary for multple languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'why', 'd', 'doing', 'those', 'needn', 'very', 'from', 'as', 'any', 'our', 'didn', 'because', 'isn', \"needn't\", 'herself', \"mightn't\", 'myself', 'be', 'is', 'after', 'what', \"weren't\", 'wouldn', 'its', 'off', 'or', 'ourselves', 'she', 'into', 've', 'i', 'their', 'weren', 'against', \"wouldn't\", 'same', \"didn't\", 'about', 'wasn', 'at', 'ours', 'all', \"shan't\", \"couldn't\", 'until', 'aren', 'hasn', 'hers', 'were', \"you're\", 'through', 'above', \"you'll\", 'few', 'theirs', 't', 'now', \"haven't\", 'to', \"should've\", 'under', 'them', 'such', 'by', 'should', 'once', 'up', 'only', 'here', 'in', 'the', 'shouldn', 'himself', 'whom', \"shouldn't\", 'between', 'your', 'most', 'can', 'haven', 'with', 'further', 'won', 'there', \"that'll\", 'couldn', 'mightn', 'am', 'than', \"wasn't\", 'had', 'been', 'during', 'are', 'y', 'no', \"hadn't\", 'being', 'and', 'did', 'a', 'has', \"doesn't\", 'ain', 'doesn', 'mustn', 'his', 'if', 'itself', 'will', 'we', \"don't\", 'of', 'that', 'him', 'on', 'me', \"isn't\", 'yours', 'who', 'again', 's', \"mustn't\", 'yourselves', 'when', 'both', 'some', 'have', 'before', 'below', 're', \"you'd\", 'you', 'do', 'then', 'an', 'he', 'm', 'for', 'down', 'll', 'o', 'my', 'so', 'don', \"hasn't\", 'hadn', 'it', 'other', 'her', 'themselves', 'over', 'does', \"you've\", 'own', \"she's\", 'was', 'how', \"aren't\", 'each', \"won't\", 'where', 'ma', 'which', 'yourself', 'more', 'having', 'too', 'these', 'shan', \"it's\", 'while', 'but', 'not', 'out', 'they', 'this', 'just', 'nor'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To', 'understand', \"'\", 'done', ',', 'let', \"'\", 'tokenize', 'first', 'sentence', '.', 'Don', \"'\", 'forget', 'special', 'cases', ',', 'like', 'U.S.A.', 'This', 'important', 'understanding', ',', 'specially', 'learning']\n"
     ]
    }
   ],
   "source": [
    "tokens_nostop = [i for i in tokens if not i in stop_words]\n",
    "print (tokens_nostop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is the process of keeping just the root (or stem) of a word. \n",
    "The reason why we stem is to shorten the lookup, and normalize sentences. Many variations of words carry the same meaning, so we can treat these variations as onde single case. For instance:\n",
    "\n",
    "* I was taking a ride in the car.\n",
    "* I was riding in the car. \n",
    "\n",
    "Both sentences carry the same meaning, so the variation in the verb ride can be stemmed so we treat **ride** and **riding** as the same word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study : studi\n",
      "studying : studi\n",
      "studied : studi\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer= PorterStemmer()\n",
    "\n",
    "print(\"study :\", stemmer.stem(\"study\")) \n",
    "print(\"studying :\", stemmer.stem(\"studying\"))\n",
    "print(\"studied :\", stemmer.stem(\"studied\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To', 'understand', \"'\", 'done', ',', 'let', \"'\", 'token', 'first', 'sentenc', '.', 'don', \"'\", 'forget', 'special', 'case', ',', 'like', 'u.s.a.', 'thi', 'import', 'understand', ',', 'special', 'learn']\n"
     ]
    }
   ],
   "source": [
    "tokens_stem = [stemmer.stem(word) for word in tokens_nostop]\n",
    "print(tokens_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of lemming, like stemming, is to reduce variations on the same words. As opposed to stemming, lemming does not simply chop off inflections. Instead it uses lexical knowledge bases and built in dictionaries to get the correct base forms of words.\n",
    "When dealing with irregular verbs, lemmatization is able to find the verb original form, while stemming is not.  \n",
    "One of the downsides of lemmatization is that we have to inform the function the Part of Speech (POS) for the term. POS is the role of the term in the sentence, usually: noun, verb, adjective, adverb, and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better (without POS): better\n",
      "better (with POS): good\n",
      "are (without POS): are\n",
      "are (with POS): be\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\")) \n",
    "print(\"better (without POS):\", lemmatizer.lemmatize(\"better\")) \n",
    "print(\"better (with POS):\", lemmatizer.lemmatize(\"better\", pos =\"a\")) \n",
    "print(\"are (without POS):\", lemmatizer.lemmatize(\"are\"))\n",
    "print(\"are (with POS):\", lemmatizer.lemmatize(\"are\", pos =\"v\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To', 'understand', 'done,', \"let'\", 'token', 'first', 'sentence.', \"don't\", 'forget', 'special', 'cases,', 'like', 'u.s.a.', 'thi', 'import', 'better', 'understanding,', 'special', 'learn']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "tokens_lemma = [lemmatizer.lemmatize(word) for word in tokens_stem]\n",
    "print (tokens_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can imagine, it is not possible to manually provide the corrent POS tag for every word. The NLTK package has the nltk.pos_tag() function, that can give us that information.\n",
    "But it gives ina  different format that the one lemmatize expect. So we build a function that makes the translation to the correct POS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To', 'understand', 'done,', \"let'\", 'token', 'first', 'sentence.', \"don't\", 'forget', 'special', 'cases,', 'like', 'u.s.a.', 'thi', 'import', 'well', 'understanding,', 'special', 'learn']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Init Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize a Sentence with the appropriate POS tag\n",
    "tokens_lemma = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens_stem]\n",
    "print (tokens_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Useful Links**\n",
    "\n",
    "Pre-processing:\n",
    "* https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html  \n",
    "* https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908  \n",
    "* https://www.researchgate.net/publication/273127322_Preprocessing_Techniques_for_Text_Mining  \n",
    " \n",
    "Lemmatization:\n",
    "\n",
    "* https://www.machinelearningplus.com/nlp/lemmatization-examples-python/  \n",
    "* https://www.geeksforgeeks.org/python-lemmatization-with-nltk/ \n",
    "* https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html \n",
    "* https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all that we saw unil now, we can build a function that normalizes a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens_nostop = [i for i in tokens if not i in stop_words]\n",
    "    tokens_stem = [stemmer.stem(word) for word in tokens_nostop]\n",
    "    tokens_lemma = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens_stem]\n",
    "    return tokens_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['intellig', 'smart', ',', 'smarter', ',', 'mayb', 'well']\n"
     ]
    }
   ],
   "source": [
    "input_str=\"He is very intelligent and smart, smarter than me, maybe better\"\n",
    "words = normalize(input_str)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SpaCy](https://spacy.io/) is, like NLTK, a package focused on text mining. One of the main differences between them is that the spaCy package has a built-in text processor that combines all the text preprocessing tasks. That means that with fewer lines of code, we can accomplish the same result that we get with NLTK. That means that spaCy is slower if we want to do just one task (tokenize, for instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To\t\t0\tto\tFalse\tFalse\tTrue\tXx\tPART\n",
      "understand\t\t3\tunderstand\tFalse\tFalse\tFalse\txxxx\tVERB\n",
      "how\t\t14\thow\tFalse\tFalse\tTrue\txxx\tADV\n",
      "it\t\t18\t-PRON-\tFalse\tFalse\tTrue\txx\tPRON\n",
      "'s\t\t20\tbe\tFalse\tFalse\tTrue\t'x\tVERB\n",
      "done\t\t23\tdo\tFalse\tFalse\tTrue\txxxx\tVERB\n",
      ",\t\t27\t,\tTrue\tFalse\tFalse\t,\tPUNCT\n",
      "let\t\t29\tlet\tFalse\tFalse\tFalse\txxx\tVERB\n",
      "'s\t\t32\t-PRON-\tFalse\tFalse\tTrue\t'x\tPRON\n",
      "tokenize\t\t35\ttokenize\tFalse\tFalse\tFalse\txxxx\tVERB\n",
      "our\t\t44\t-PRON-\tFalse\tFalse\tTrue\txxx\tDET\n",
      "first\t\t48\tfirst\tFalse\tFalse\tTrue\txxxx\tADJ\n",
      "sentence\t\t54\tsentence\tFalse\tFalse\tFalse\txxxx\tNOUN\n",
      ".\t\t62\t.\tTrue\tFalse\tFalse\t.\tPUNCT\n",
      "Do\t\t64\tdo\tFalse\tFalse\tTrue\tXx\tVERB\n",
      "n't\t\t66\tnot\tFalse\tFalse\tTrue\tx'x\tADV\n",
      "forget\t\t70\tforget\tFalse\tFalse\tFalse\txxxx\tVERB\n",
      "special\t\t77\tspecial\tFalse\tFalse\tFalse\txxxx\tADJ\n",
      "cases\t\t85\tcase\tFalse\tFalse\tFalse\txxxx\tNOUN\n",
      ",\t\t90\t,\tTrue\tFalse\tFalse\t,\tPUNCT\n",
      "like\t\t92\tlike\tFalse\tFalse\tFalse\txxxx\tADP\n",
      "U.S.A.\t\t97\tU.S.A.\tFalse\tFalse\tFalse\tX.X.X.\tPROPN\n",
      "This\t\t104\tThis\tFalse\tFalse\tTrue\tXxxx\tDET\n",
      "is\t\t109\tbe\tFalse\tFalse\tTrue\txx\tVERB\n",
      "important\t\t112\timportant\tFalse\tFalse\tFalse\txxxx\tADJ\n",
      "for\t\t122\tfor\tFalse\tFalse\tTrue\txxx\tADP\n",
      "understanding\t\t126\tunderstanding\tFalse\tFalse\tFalse\txxxx\tNOUN\n",
      ",\t\t139\t,\tTrue\tFalse\tFalse\t,\tPUNCT\n",
      "specially\t\t141\tspecially\tFalse\tFalse\tFalse\txxxx\tADV\n",
      "when\t\t151\twhen\tFalse\tFalse\tTrue\txxxx\tADV\n",
      "learning\t\t156\tlearn\tFalse\tFalse\tFalse\txxxx\tVERB\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import en_core_web_sm #English Text processor\n",
    "\n",
    "nlp = en_core_web_sm.load() #Loading English Text processor\n",
    "doc = nlp(sample_text) #Processing text\n",
    "\n",
    "for token in doc:\n",
    "    print(\"{0}\\t\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\".format(\n",
    "        token.text,\n",
    "        token.idx,\n",
    "        token.lemma_,\n",
    "        token.is_punct,\n",
    "        token.is_space,\n",
    "        token.is_stop,\n",
    "        token.shape_,\n",
    "        token.pos_,\n",
    "        token.tag_\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more differences between these packages, that we won't cover in this material, but can be found in the Useful links  below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Useful Links**\n",
    "\n",
    "Spacy:  \n",
    "\n",
    "* https://nlpforhackers.io/complete-guide-to-spacy/ \n",
    "\n",
    "NLTK vs Spacy:\n",
    "\n",
    "* https://www.oreilly.com/learning/how-can-i-tokenize-a-sentence-with-python  \n",
    "* https://medium.com/@akankshamalhotra24/introduction-to-libraries-of-nlp-in-python-nltk-vs-spacy-42d7b2f128f2  \n",
    "* https://medium.com/@pemagrg/private-nltk-vs-spacy-3926b3674ee4  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play\n",
    "\n",
    "Rewrite the normalize function, but using the spaCy package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Term Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can start analyzing our text. The first thing we can think about it is counting words. \n",
    "\n",
    "A Document Term Matrix (DTM) is a mathematical matrix that shows the frequency of a token for a given document. In a DTM, rows correspond to documents and columns correspond to tokens.\n",
    "\n",
    "This representation is useful because it turns the task of comparing two texts into comparing two vectors (two rows in the DTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   another  is  sample  sentence  text  the  third  this\n",
      "0        0   1       1         0     1    0      0     1\n",
      "1        1   1       1         0     0    0      0     1\n",
      "2        0   1       0         1     0    1      1     1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data = [\"This is a sample text\",\n",
    "        \"This is another sample\",\n",
    "        \"This is the third sentence\"]\n",
    "\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(data)\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this concept has a flaw. When comparing documents in the same subject, the task of differentiating them can be difficult for they share a lot of the same terms.  \n",
    "For example: Mathmatical papers, they all share words as formula, theory, calculation, etc.\n",
    "\n",
    "A possible solution for that is TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Useful links**\n",
    "* [DTM with Pandas and Sklearn](https://stackoverflow.com/questions/15899861/efficient-term-document-matrix-with-nltk)* https://datawarrior.wordpress.com/2018/01/22/document-term-matrix-text-mining-in-r-and-python/  \n",
    "* https://markroxor.github.io/gensim/static/notebooks/dtm_example.html  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a collection of document, how can we identify the keywords for each one of them?\n",
    "\n",
    "The TF-IDF was created with that in mind. TF-IDF stands for:\n",
    "* TF - Term Frequency\n",
    "* IDF - Inverse Document Frequency\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{equation*}\n",
    "tf(t) = \\frac{\\text{Number of times term t appears in a document}}{\\text{Total number of terms in the document}} \\\\\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "idf(t) = \\log \\frac{\\text{Total number of documents}}{\\text{Number of documents with term t in it}} \\\\\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "tf\\text{-}idf(t) = tf(t) \\cdot idf(t) \\\\\n",
    "\\end{equation*}\n",
    "\n",
    "In this equation, a term has a high TF-IDF if it appears a lot in a document (TF), but is penalized if also appears im multiple documents (IDF). With that, we assure that only the most commom and unique words in the document have a high weight.  \n",
    "Now, we can reproduce the DTM table, but instead of having a simple count, we will have the TF-IDF score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   another        is    sample  sentence     text      the    third      this\n",
      "0  0.00000  0.391484  0.504107   0.00000  0.66284  0.00000  0.00000  0.391484\n",
      "1  0.66284  0.391484  0.504107   0.00000  0.00000  0.00000  0.00000  0.391484\n",
      "2  0.00000  0.307144  0.000000   0.52004  0.00000  0.52004  0.52004  0.307144\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "Y = tfidf_transformer.fit_transform(X)\n",
    "df = pd.DataFrame(Y.toarray(), columns=vec.get_feature_names())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Useful links**\n",
    "* [TF-IDF](http://www.tfidf.com/)\n",
    "* [A Simple Probabilistic Explanation of TF-IDF Heuristic](https://digitalcommons.utep.edu/cgi/viewcontent.cgi?article=1852&context=cs_techrep)\n",
    "* [Keyword Extraction with TF-IDF and scikit-learn ](http://kavita-ganesan.com/extracting-keywords-from-text-tfidf/#.XKNh2mczWig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Project Gutenberg](http://www.gutenberg.org/) is a website that provides free access to over 58,000 free eBooks which are in public domain. \n",
    "\n",
    "We are going to work with the book [The Adventures of Sherlock Holmes](http://www.gutenberg.org/files/1661/1661-h/1661-h.htm), by Sir Arthur Conan Doyle, available on the website. This book is a collection of short stories.\n",
    "\n",
    "Our objective is to find the keywords for each short story.\n",
    "\n",
    "To accomplish that, we need to:\n",
    "1. Download the book\n",
    "3. Preprocess the text\n",
    "4. Calculate TF-IDF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Host)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
